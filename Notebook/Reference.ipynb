{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1810f8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 12 Day 1</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 20 November 2022</div>\n",
    "\n",
    "# Final Project\n",
    "Your final project will be to pick a corpus of text to analyse, model, and summarize. You need to use one or more of the ten methods exemplified in this notebook, and extend them as you see fit, possibly repeating methodologies you encounter in your research on the Web. You can use different methodologies and different libraries, but you may not pick a notebook you find on the Web and present that as yours. You may only pick the text you will work with. The techniques you employ in your final project need to begin from the ones in this notebook.\n",
    "\n",
    "You will ***have to*** implement at least ***one*** computational model (a.k.a. Bayesian simulation), ***one*** network model (a.k.a. PageRank), and ***one*** semantic model (e.g. clustering or LDA). Note that all groups will start form the same base 3 methods, so for a good peer grade, you definitely need to do research and extend the methods or apply them creatively in order to extreact a maximum amount of information from your chosen corpus.\n",
    "\n",
    "The main libraries in NLP are `nltk`, `spacy`, `gensim`, `sklearn`, and a few others.\n",
    "\n",
    "```\n",
    "pip install spacy==2.2.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31b13ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e12cc",
   "metadata": {},
   "source": [
    "# 1. Pretrained sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3c9267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I am a fan of Max Verstappen and in love with Dua Lipa\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75240138",
   "metadata": {},
   "source": [
    "## NLTK/Vader\n",
    "NLTK’s Vader sentiment analysis tool uses a bag of words approach (a lookup table of positive and negative words) with some simple heuristics (e.g. increasing the intensity of the sentiment if some words like “really”, “so” or “a bit” are present).\n",
    "\n",
    "Sentences containing negated positive words (e.g. “not happy”, “not good”) will still receive a negative sentence sentiment. Simpler sentiment analysis tools will just take the average of the sentiments of the words and would miss subtle details like this.\n",
    "\n",
    "The disadvantage of this approach is that Out of Vocab (OOV) words that the sentiment analysis tool has not seen before will not be classified as positive/negative (e.g. typos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ec74f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/liviagonsalves/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.581, 'pos': 0.419, 'compound': 0.7579}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sid.polarity_scores(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "caafbcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.581, 'pos': 0.419, 'compound': 0.7579}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e4b33a",
   "metadata": {},
   "source": [
    "## TextBlob\n",
    "Textblob’s Sentiment Analysis works in a similar way to NLTK — using a bag of words classifier, but the advantage is that it includes Subjectivity Analysis too (how factual/opinionated a piece of text is)!\n",
    "\n",
    "However, it doesn’t contain the heuristics that NLTK has, and so it won’t intensify or negate a sentence’s sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "715b0147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5, subjectivity=0.6)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "TextBlob(sentence).sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08997740",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "644c818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def eda(sentences):\n",
    "    processed_sentences = []\n",
    "\n",
    "    for s in sentences:\n",
    "        # Remove all the special characters\n",
    "        processed_sentence = re.sub(r'\\W', ' ', str(s))\n",
    "\n",
    "        # remove all single characters\n",
    "        processed_sentence= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_sentence)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        processed_sentence = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_sentence) \n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        processed_sentence = re.sub(r'\\s+', ' ', processed_sentence, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        processed_sentence = re.sub(r'^b\\s+', '', processed_sentence)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        processed_sentence = processed_sentence.lower()\n",
    "\n",
    "        processed_sentences.append(processed_sentence)\n",
    "        \n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c4f8d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Shall I compare thee to a summer’s day?\",\n",
    "\"Thou art more lovely and more temperate:\",\n",
    "\"Rough winds do shake the darling buds of May,\",\n",
    "\"And summer’s lease hath all too short a date:\",\n",
    "\"Sometime too hot the eye of heaven shines,\",\n",
    "\"And often is his gold complexion dimm’d;\",\n",
    "\"And every fair from fair sometime declines,\",\n",
    "\"By chance or nature’s changing course untrimm’d;\",\n",
    "\"But thy eternal summer shall not fade\",\n",
    "\"Nor lose possession of that fair thou owest;\",\n",
    "\"Nor shall Death brag thou wander’st in his shade,\",\n",
    "\"When in eternal lines to time thou growest:\",\n",
    "\"So long as men can breathe or eyes can see,\",\n",
    "\"So long lives this and this gives life to thee.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e514659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35478ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sentences = eda(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e99c379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shall compare thee to summer day ',\n",
       " 'thou art more lovely and more temperate ',\n",
       " 'rough winds do shake the darling buds of may ',\n",
       " 'and summer lease hath all too short date ',\n",
       " 'sometime too hot the eye of heaven shines ',\n",
       " 'and often is his gold complexion dimm ',\n",
       " 'and every fair from fair sometime declines ',\n",
       " 'by chance or nature changing course untrimm ',\n",
       " 'but thy eternal summer shall not fade',\n",
       " 'nor lose possession of that fair thou owest ',\n",
       " 'nor shall death brag thou wander st in his shade ',\n",
       " 'when in eternal lines to time thou growest ',\n",
       " 'so long as men can breathe or eyes can see ',\n",
       " 'so long lives this and this gives life to thee ']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42705702",
   "metadata": {},
   "source": [
    "Scikit-Learn library contains the TfidfVectorizer class that can be used to convert text features into TF-IDF feature vectors. The following script performs this.\n",
    "\n",
    "`max_df` specifies that only use those words that occur in a maximum of 80% of the documents. Words that occur in all documents are too common and are not very useful for classification. Similarly, `min-df`, when set to `p`, includes words that occur in at least `p` documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d2a50bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer (max_features=2500, min_df=1, max_df=0.8, stop_words=stopwords.words('english'))\n",
    "processed_sentences_vectors = vectorizer.fit_transform(processed_sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7b03e4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 58)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentences_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b82b15",
   "metadata": {},
   "source": [
    "We converted the data into the numeric form. As the last step before we train our algorithms, we need to divide our data into training and testing sets. The training set will be used to train the algorithm while the test set will be used to evaluate the performance of the machine learning model.\n",
    "\n",
    "We now use the train_test_split class from the sklearn.model_selection module to divide our data into training and testing set. The method takes the feature set as the first parameter, the label set as the second parameter, and a value for the test_size parameter. We specified a value of 0.2 for test_size which means that our data set will be split into two sets of 80% and 20% data. We will use the 80% dataset for training and 20% dataset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a8574cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_sentences_vectors, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4909b",
   "metadata": {},
   "source": [
    "Once data is split into training and test set, machine learning algorithms can be used to learn from the training data. You can use any machine learning algorithm. However, we will use the Random Forest algorithm, owing to its ability to act upon non-normalized data.\n",
    "\n",
    "The sklearn.ensemble module contains the RandomForestClassifier class that can be used to train the machine learning model using the random forest algorithm. To do so, we need to call the fit method on the RandomForestClassifier class and pass it our training features and labels, as parameters. Look at the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dbe652c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae664d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet33 = [\n",
    "\"Full many a glorious morning have I seen\",\n",
    "\"Flatter the mountain tops with sovereign eye,\",\n",
    "\"Kissing with golden face the meadows green,\",\n",
    "\"Gilding pale streams with heavenly alchemy;\",\n",
    "\"Anon permit the basest clouds to ride\",\n",
    "\"With ugly rack on his celestial face,\",\n",
    "\"And from the forlorn world his visage hide,\",\n",
    "\"Stealing unseen to west with this disgrace:\",\n",
    "\"Even so my sun one early morn did shine,\",\n",
    "\"With all triumphant splendour on my brow;\",\n",
    "\"But out, alack, he was but one hour mine,\",\n",
    "\"The region cloud hath mask’d him from me now.\",\n",
    "\"Yet him for this my love no whit disdaineth;\",\n",
    "\"Suns of the world may stain when heaven’s sun staineth.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0f1212e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sonnet33 = eda(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b057c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sonnet33_vectors = vectorizer.fit_transform(processed_sonnet33).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c7a60d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 58)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sonnet33_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dfbd34b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = text_classifier.predict(processed_sonnet33_vectors[:,:58])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f1311c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfdf872",
   "metadata": {},
   "source": [
    "Finally, to evaluate the performance of the machine learning models, we can use classification metrics such as a confusion metrix, F1 measure, accuracy, etc.\n",
    "\n",
    "To find the values for these metrics, we can use classification_report, confusion_matrix, and accuracy_score utilities from the sklearn.metrics library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29925d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [5 6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.33      0.22         3\n",
      "           1       0.75      0.55      0.63        11\n",
      "\n",
      "    accuracy                           0.50        14\n",
      "   macro avg       0.46      0.44      0.43        14\n",
      "weighted avg       0.62      0.50      0.54        14\n",
      "\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_labels, predictions))\n",
    "print(classification_report(y_labels, predictions))\n",
    "print(accuracy_score(y_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e6492f",
   "metadata": {},
   "source": [
    "92% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0a5271",
   "metadata": {},
   "source": [
    "## Text2Emotion\n",
    "```\n",
    "pip install text2emotion\n",
    "```\n",
    "\n",
    "Positive/negative is not the only possible sentiment classification!\n",
    "\n",
    "- [text2emotion](https://pypi.org/project/text2emotion/)\n",
    "- [github](https://shivamsharma26.github.io/text2emotion/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e5293edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import text2emotion as te\n",
    "\n",
    "text = \"\"\"I was asked to sign a third party contract a week out from stay. \n",
    "If it wasn't an 8 person group that took a lot of wrangling I would have cancelled the booking straight away. \n",
    "Bathrooms - there are no stand alone bathrooms. Please consider this - you have to clear out the main bedroom \n",
    "to use that bathroom. Other option is you walk through a different bedroom to get to its en-suite. \n",
    "Signs all over the apartment - there are signs everywhere - some helpful - some telling you rules. \n",
    "Perhaps some people like this but It negatively affected our enjoyment of the accommodation. \n",
    "Stairs - lots of them - some had slightly bending wood which caused a minor injury.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "04a8e16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/liviagonsalves/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "31efcadf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'emoji' has no attribute 'UNICODE_EMOJI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5c/lkkhjt_s43b_77vc0h_w1bpw0000gn/T/ipykernel_3118/407489403.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_emotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/text2emotion/__init__.py\u001b[0m in \u001b[0;36mget_emotion\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   2714\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2716\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2717\u001b[0m     \u001b[0memotion_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2718\u001b[0m     \u001b[0memotions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Happy\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Angry\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Surprise\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sad\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Fear\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/text2emotion/__init__.py\u001b[0m in \u001b[0;36mcleaning\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m   2698\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2700\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memojis_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2701\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'http\\S+|www.\\S+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremoving_contradictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/text2emotion/__init__.py\u001b[0m in \u001b[0;36memojis_extractor\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m   2567\u001b[0m                             \u001b[0;34m'Fear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2568\u001b[0m                             'Fear']}\n\u001b[0;32m-> 2569\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memoji\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNICODE_EMOJI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2570\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2571\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/text2emotion/__init__.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2567\u001b[0m                             \u001b[0;34m'Fear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2568\u001b[0m                             'Fear']}\n\u001b[0;32m-> 2569\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memoji\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNICODE_EMOJI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2570\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2571\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'emoji' has no attribute 'UNICODE_EMOJI'"
     ]
    }
   ],
   "source": [
    "te.get_emotion(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "te.get_emotion(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d7df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "te.get_emotion('.'.join(sonnet33))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d8202",
   "metadata": {},
   "source": [
    "## Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cf705282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fc5264ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5c/lkkhjt_s43b_77vc0h_w1bpw0000gn/T/ipykernel_3118/1392401143.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \"\"\"\n\u001b[0;32m---> 54\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04a25cc",
   "metadata": {},
   "source": [
    "```\n",
    "pip install --user spacytextblob==0.1.3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacytextblob insists to install spacy==3.4.3, which causes me problems!\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16733411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.add_pipe(\"spacytextblob\")\n",
    "nlp.add_pipe(SpacyTextBlob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d0f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I had a really horrible day. It was the worst day ever! But every now and then I have a really good day that makes me happy.'\n",
    "doc = nlp(text)\n",
    "doc._.blob.polarity                            # Polarity: -0.125\n",
    "doc._.blob.subjectivity                        # Subjectivity: 0.9\n",
    "doc._.blob.sentiment_assessments.assessments   # Assessments: [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]\n",
    "doc._.blob.ngrams()\n",
    "#doc.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b06a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.blob.polarity                            # Polarity: -0.125\n",
    "doc._.blob.subjectivity                        # Subjectivity: 0.9\n",
    "doc._.blob.sentiment_assessments.assessments   # Assessments: [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]\n",
    "doc._.blob.ngrams()\n",
    "#doc.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8616363",
   "metadata": {},
   "source": [
    "# 2. Computational statistics with the Negative binomial\n",
    "The [negative binomial](https://en.wikipedia.org/wiki/Negative_binomial_distribution) distribution is the distribution of a random variable that is defined as the number of independent Bernoulli trials until the k-th “success”. In short, we repeat a Bernoulli experiment until we observe k successes and record the number of trials it required.\n",
    " \n",
    "$$ Y = \\text{NB}(k, p)$$\n",
    " \n",
    "where $0<=p<=1$ is the probability of success in each Bernoulli trial, , $k > 0$ usually integer, and $y \\in \\{k, k+1, \\cdot \\}$\n",
    "\n",
    "Let’s understand the concept with the example of tossing a coin. Let’s say we want to continue flipping the coin until 3 heads come. This experiment of flipping a coin until 3 heads (r=3) come can be called as negative binomial experiment. And, the number of times the coin need to be flipped in each experiment represent the value of negative binomial random variable, X.\n",
    "\n",
    "The probability mass function (pmf) is\n",
    "\n",
    "$$p(y \\;|\\;k,p) = \\binom{y-1}{y-k}(1-p)^{y-k}p^k$$\n",
    "\n",
    "Recall we aim to have $k$ successes. And success is one of the two possible outcomes of a trial, so the number of trials can never be smaller than the number of successes. Thus, we can be confident to say that $y >=k$.\n",
    "\n",
    "But this is not the only way of defining the negative binomial distribution, there are plenty of options! One of the most interesting, and the one you see in PyMC3, is as a **continuous mixture**. The negative binomial distribution describes a Poisson random variable whose rate is also a random variable (not a fixed constant!) following a gamma distribution. Or in other words, conditional on a gamma-distributed variable $\\mu$, the variable $$Y has a Poisson distribution with mean $\\mu$.\n",
    "\n",
    "Under this alternative definition, the pmf is\n",
    "\n",
    "$$p(y \\;|\\;k,p)=\\binom{y+\\alpha -1}{y} \\left(\\frac{\\alpha}{\\mu +\\alpha}\\right)^\\alpha \\left(\\frac{\\mu}{\\mu +\\alpha}\\right)^y$$\n",
    "\n",
    "where $\\mu$ is the parameter of the Poisson distribution (the mean, and variance too!) and $\\alpha$ is the rate parameter of the gamma.\n",
    "\n",
    "In SciPy, the definition of the negative binomial distribution differs a little from the one in the introduction. They define $Y$ = Number of failures until $k$ sucesses and then $y$ starts at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e98d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import nbinom\n",
    "\n",
    "y = np.arange(0, 30)\n",
    "k = 3\n",
    "p1 = 0.5\n",
    "p2 = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e0396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "\n",
    "ax[0].bar(y, nbinom.pmf(y, k, p1))\n",
    "ax[0].set_xticks(np.linspace(0, 30, num=11))\n",
    "ax[0].set_title(f\"k = {k}, p = {p1}\")\n",
    "\n",
    "ax[1].bar(y, nbinom.pmf(y, k, p2))\n",
    "ax[1].set_xticks(np.linspace(0, 30, num=11))\n",
    "ax[1].set_title(f\"k = {k}, p = {p2}\")\n",
    "\n",
    "fig.suptitle(\"Y = Number of failures until k successes\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4826fa7",
   "metadata": {},
   "source": [
    "For example, when $p=0.5$, the probability of seeing $y=0$ failures before 3 successes (or in other words, the probability of having 3 successes out of 3 trials) is 0.125, and the probability of seeing $y=3$ failures before 3 successes is 0.156."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb20c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nbinom.pmf(y, k, p1)[0])\n",
    "print(nbinom.pmf(y, k, p1)[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16040dcc",
   "metadata": {},
   "source": [
    "if one wants to show this probability mass function as if we are following the first definition of negative binomial distribution we introduced, we just need to shift the whole thing to the right by adding $k$ to the $y$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a70cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "\n",
    "ax[0].bar(y + k, nbinom.pmf(y, k, p1))\n",
    "ax[0].set_xticks(np.linspace(3, 30, num=10))\n",
    "ax[0].set_title(f\"k = {k}, p = {p1}\")\n",
    "\n",
    "ax[1].bar(y + k, nbinom.pmf(y, k, p2))\n",
    "ax[1].set_xticks(np.linspace(3, 30, num=10))\n",
    "ax[1].set_title(f\"k = {k}, p = {p2}\")\n",
    "\n",
    "fig.suptitle(\"Y = Number of trials until k successes\", fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ce370",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.stats import nbinom \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# \n",
    "# X = Discrete negative binomial random variable representing number of sales call required to get r=3 leads \n",
    "# P = Probability of successful sales call \n",
    "# \n",
    "\n",
    "X = np.arange(3, 30) \n",
    "r = 3 \n",
    "P = 0.1 \n",
    "\n",
    "# \n",
    "# Calculate geometric probability distribution \n",
    "# \n",
    "nbinom_pd = nbinom.pmf(X, r, P) \n",
    "\n",
    "# \n",
    "# Plot the probability distribution \n",
    "# \n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6)) \n",
    "ax.plot(X, nbinom_pd, 'bo', ms=8, label='nbinom pmf') \n",
    "ax.plot(X, nbinom_pd, 'bo', ms=8, label='nbinom pmf') \n",
    "plt.ylabel(\"Probability\", fontsize=\"18\") \n",
    "plt.xlabel(\"X - No. of Sales Call\", fontsize=\"18\") \n",
    "plt.title(\"Negative Binomial Distribution - No. of Sales Call Vs Probability\", fontsize=\"18\") \n",
    "ax.vlines(X, 0, nbinom_pd, colors='b', lw=5, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d309fc",
   "metadata": {},
   "source": [
    "## Utility routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0eb826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traces(trcs, varnames=None):\n",
    "    '''Plot traces with overlaid means and values'''\n",
    "\n",
    "    nrows = len(trcs.varnames)\n",
    "    if varnames is not None:\n",
    "        nrows = len(varnames)\n",
    "\n",
    "    ax = pm.traceplot(trcs, varnames=varnames, figsize=(12,nrows*1.4),\n",
    "                      lines={k: v['mean'] for k, v in\n",
    "                             pm.summary(trcs,varnames=varnames).iterrows()})\n",
    "\n",
    "    for i, mn in enumerate(pm.summary(trcs, varnames=varnames)['mean']):\n",
    "        ax[i,0].annotate('{:.2f}'.format(mn), xy=(mn,0), xycoords='data',\n",
    "                         xytext=(5,10), textcoords='offset points', rotation=90,\n",
    "                         va='bottom', fontsize='large', color='#AA0022')\n",
    "\n",
    "def strip_derived_rvs(rvs):\n",
    "    '''Remove PyMC3-generated RVs from a list'''\n",
    "\n",
    "    ret_rvs = []\n",
    "    for rv in rvs:\n",
    "        if not (re.search('_log',rv.name) or re.search('_interval',rv.name)):\n",
    "            ret_rvs.append(rv)\n",
    "    return ret_rvs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8347468",
   "metadata": {},
   "source": [
    "## Bayesian modeling of Sneezing with Negative Binomial distribution\n",
    "We model sneezing while drinking alcohol and taking antihistamines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6796a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc2b54d",
   "metadata": {},
   "source": [
    "Assume that sneezing occurs at some baseline rate, and that consuming alcohol, not taking antihistamines, or doing both, increase its frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b8fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Poisson values\n",
    "theta_noalcohol_meds = 1    # no alcohol, took an antihist\n",
    "theta_alcohol_meds = 3      # alcohol, took an antihist\n",
    "theta_noalcohol_nomeds = 6  # no alcohol, no antihist\n",
    "theta_alcohol_nomeds = 36   # alcohol, no antihist\n",
    "\n",
    "# Create samples\n",
    "q = 1000\n",
    "df_pois = pd.DataFrame({\n",
    "        'nsneeze': np.concatenate((np.random.poisson(theta_noalcohol_meds, q),\n",
    "                                   np.random.poisson(theta_alcohol_meds, q),\n",
    "                                   np.random.poisson(theta_noalcohol_nomeds, q),\n",
    "                                   np.random.poisson(theta_alcohol_nomeds, q))),\n",
    "        'alcohol': np.concatenate((np.repeat(False, q),\n",
    "                                   np.repeat(True, q),\n",
    "                                   np.repeat(False, q),\n",
    "                                   np.repeat(True, q))),\n",
    "        'nomeds': np.concatenate((np.repeat(False, q),\n",
    "                                      np.repeat(False, q),\n",
    "                                      np.repeat(True, q),\n",
    "                                      np.repeat(True, q)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b021f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pois.groupby(['nomeds', 'alcohol'])['nsneeze'].agg(['mean', 'var'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914d3af",
   "metadata": {},
   "source": [
    "Since the mean and variance of a Poisson distributed random variable are equal, the sample means and variances are very close.\n",
    "\n",
    "### Negative Binomial Data\n",
    "Now, suppose every subject in the dataset had the flu, increasing the variance of their sneezing (and causing an unfortunate few to sneeze over 70 times a day). If the mean number of sneezes stays the same but variance increases, the data might follow a negative binomial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma shape parameter\n",
    "alpha = 10\n",
    "\n",
    "def get_nb_vals(mu, alpha, size):\n",
    "    \"\"\"Generate negative binomially distributed samples by\n",
    "    drawing a sample from a gamma distribution with mean `mu` and\n",
    "    shape parameter `alpha', then drawing from a Poisson\n",
    "    distribution whose rate parameter is given by the sampled\n",
    "    gamma variable.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    g = stats.gamma.rvs(alpha, scale=mu / alpha, size=size)\n",
    "    return stats.poisson.rvs(g)\n",
    "\n",
    "# Create samples\n",
    "n = 1000\n",
    "df = pd.DataFrame({\n",
    "        'nsneeze': np.concatenate((get_nb_vals(theta_noalcohol_meds, alpha, n),\n",
    "                                   get_nb_vals(theta_alcohol_meds, alpha, n),\n",
    "                                   get_nb_vals(theta_noalcohol_nomeds, alpha, n),\n",
    "                                   get_nb_vals(theta_alcohol_nomeds, alpha, n))),\n",
    "        'alcohol': np.concatenate((np.repeat(False, n),\n",
    "                                   np.repeat(True, n),\n",
    "                                   np.repeat(False, n),\n",
    "                                   np.repeat(True, n))),\n",
    "        'nomeds': np.concatenate((np.repeat(False, n),\n",
    "                                      np.repeat(False, n),\n",
    "                                      np.repeat(True, n),\n",
    "                                      np.repeat(True, n)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62477bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['nomeds', 'alcohol'])['nsneeze'].agg(['mean', 'var'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd83cb",
   "metadata": {},
   "source": [
    "As in the Poisson regression example, we see that drinking alcohol and/or not taking antihistamines increase the sneezing rate to varying degrees. Unlike in that example, for each combination of `alcohol` and `nomeds`, the variance of `nsneeze` is higher than the mean. This suggests that a Poisson distrubution would be a poor fit for the data since the mean and variance of a Poisson distribution are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x='nsneeze', row='nomeds', col='alcohol', data=df, kind='count', aspect=1.5)\n",
    "\n",
    "# Make x-axis ticklabels less crowded\n",
    "ax = g.axes[1, 0]\n",
    "labels = range(len(ax.get_xticklabels(which='both')))\n",
    "ax.set_xticks(labels[::5])\n",
    "ax.set_xticklabels(labels[::5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a1ce74",
   "metadata": {},
   "source": [
    "## Negative Binomial Regression\n",
    "### Create GLM Model\n",
    "Generalized Linear Models are an advanced methodology for Bayesian simulation, becausee it makes it very simple to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf3c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fml = 'nsneeze ~ alcohol + nomeds + alcohol:nomeds'\n",
    "\n",
    "with pm.Model() as model:\n",
    "    pm.glm.GLM.from_formula(formula=fml, data=df, family=pm.glm.families.NegativeBinomial())\n",
    "\n",
    "    # Old initialization\n",
    "    # start = pm.find_MAP(fmin=optimize.fmin_powell)\n",
    "    # C = pm.approx_hessian(start)\n",
    "    # trace = pm.sample(4000, step=pm.NUTS(scaling=C))\n",
    "\n",
    "    trace = pm.sample(2000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16093479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traces(trcs, varnames=None):\n",
    "    '''Plot traces with overlaid means and values'''\n",
    "\n",
    "    nrows = len(trcs.varnames)\n",
    "    if varnames is not None:\n",
    "        nrows = len(varnames)\n",
    "\n",
    "    ax = pm.traceplot(trcs, varnames=varnames, figsize=(12,nrows*1.4),\n",
    "                      lines={k: v['mean'] for k, v in\n",
    "                             pm.summary(trcs,varnames=varnames).iterrows()})\n",
    "\n",
    "    for i, mn in enumerate(pm.summary(trcs, varnames=varnames)['mean']):\n",
    "        ax[i,0].annotate('{:.2f}'.format(mn), xy=(mn,0), xycoords='data',\n",
    "                         xytext=(5,10), textcoords='offset points', rotation=90,\n",
    "                         va='bottom', fontsize='large', color='#AA0022')\n",
    "\n",
    "def strip_derived_rvs(rvs):\n",
    "    '''Remove PyMC3-generated RVs from a list'''\n",
    "\n",
    "    ret_rvs = []\n",
    "    for rv in rvs:\n",
    "        if not (re.search('_log',rv.name) or re.search('_interval',rv.name)):\n",
    "            ret_rvs.append(rv)\n",
    "    return ret_rvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76524c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs = [rv.name for rv in strip_derived_rvs(model.unobserved_RVs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1987ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traces(trace[1000:], varnames=rvs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54034e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform coefficients to recover parameter values\n",
    "np.exp(pm.summary(trace[1000:], varnames=rvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea58f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(pm.summary(trace[1000:], varnames=rvs)[['mean','hdi_3%','hdi_97%']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b11a63",
   "metadata": {},
   "source": [
    "The mean values are close to the values we specified when generating the data: - The base rate is a constant 1. - Drinking alcohol triples the base rate. - Not taking antihistamines increases the base rate by 6 times. - Drinking alcohol and not taking antihistamines doubles the rate that would be expected if their rates were independent. If they were independent, then doing both would increase the base rate by 3 * 6 = 18 times, but instead the base rate is increased by 2.\n",
    "\n",
    "Finally, even though the sample for mu is highly skewed, its median value is close to the sample mean, and the mean of alpha is also quite close to its actual value of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7609bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(trace[1000:]['mu'], [25,50,75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nsneeze.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deea7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace[1000:]['alpha'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0238af3a",
   "metadata": {},
   "source": [
    "# 3. Mao corpus\n",
    "Harvested from [here](https://www.marxists.org/reference/archive/mao/selected-works/date-index.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5f7722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for root, dirs, files in os.walk(\"c:/Users/Dino/mao\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "             print(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "mao_paragraphs = []\n",
    "for root, dirs, files in os.walk(\"c:/Users/Dino/mao\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            print(os.path.join(root, file))\n",
    "            with open(os.path.join(root, file), \"r\") as input:\n",
    "                paragraphs = input.read().split(\"\\n\\n\")   #\\n\\n denotes there is a blank line in between paragraphs.\n",
    "            #print(paragraphs[0])\n",
    "            mao_paragraphs.extend(paragraphs)\n",
    "            \n",
    "print(len(mao_paragraphs))\n",
    "print(mao_paragraphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f297b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "mao_sentences = []\n",
    "for root, dirs, files in os.walk(\"c:/Users/Dino/mao\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            print(os.path.join(root, file))\n",
    "            with open(os.path.join(root, file), \"r\") as input:\n",
    "                sentences = input.read().split(\". \")   #. denotes end of sentence\n",
    "            mao_sentences.extend(sentences)\n",
    "            \n",
    "print(len(mao_sentences))\n",
    "print(mao_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada6133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "mao_df = pd.DataFrame(mao_sentences, columns = ['Sentence'])\n",
    "mao_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f735a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mao_df['NumWords'] = mao_df['Sentence'].apply(lambda x: len(x.split()))\n",
    "mao_df[['NumWords']].hist(figsize=(12, 6), bins=10, xlabelsize=8, ylabelsize=8);\n",
    "plt.title(\"Distribution of number of words in each sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ''.join([word for word in mao_df['Sentence']]).replace('\\n', ' ')\n",
    "all_words[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=800, height=500, max_font_size=110, background_color=\"white\", max_words=3000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(all_words)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "NON_ALPHANUM = re.compile(r'[\\W]')\n",
    "NON_ASCII = re.compile(r'[^a-z0-1\\s]')\n",
    "def normalize_texts(texts):\n",
    "    normalized_texts = ''\n",
    "    lower = texts.lower()\n",
    "    no_punctuation = NON_ALPHANUM.sub(r' ', lower)\n",
    "    no_non_ascii = NON_ASCII.sub(r'', no_punctuation)\n",
    "    return no_non_ascii\n",
    "  \n",
    "mao_df['Sentence2'] = mao_df['Sentence'].apply(normalize_texts)\n",
    "mao_df.head()\n",
    "mao_df['Sentence2'] = mao_df['Sentence2'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_top_n_words(corpus, n=10):\n",
    "    vec = CountVectorizer(stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "words = []\n",
    "word_values = []\n",
    "for i,j in get_top_n_words(mao_df['Sentence2'], 40):\n",
    "    words.append(i)\n",
    "    word_values.append(j)\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(range(len(words)), word_values);\n",
    "ax.set_xticks(range(len(words)));\n",
    "ax.set_xticklabels(words, rotation='vertical');\n",
    "ax.set_title(\"Mao's top 40 words\");\n",
    "ax.set_xlabel('Word');\n",
    "ax.set_ylabel('Number of occurences');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984beda2",
   "metadata": {},
   "source": [
    "# 4. Main characters\n",
    "Let's extract all characters, visualize their occurences relative to their position in the corpus, find out what adjectives they are associated with, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20af89d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Process `text` with Spacy NLP Parser\n",
    "text = '. '.join(mao_sentences)\n",
    "processed_text = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0223269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many sentences are in the book (Pride & Prejudice)?\n",
    "sentences = [s for s in processed_text.sents]\n",
    "print(len(sentences))\n",
    "\n",
    "# Print sentences from index 10 to index 15, to make sure that we have parsed the correct book\n",
    "print(sentences[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the personal names and count their occurrences. \n",
    "# Expected output is a list in the following form: [('xxx', 622), ('yyy', 312), ('zzz', 286), ...].\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def find_character_occurences(doc):\n",
    "    \"\"\"\n",
    "    Return a list of actors from `doc` with corresponding occurences.\n",
    "    \n",
    "    :param doc: Spacy NLP parsed document\n",
    "    :return: list of tuples in form\n",
    "        [('xxx', 622), ('yyy', 312), ('zzz', 286), ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    characters = Counter()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            characters[ent.lemma_] += 1\n",
    "            \n",
    "    return characters.most_common()\n",
    "\n",
    "print(find_character_occurences(processed_text)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2121a90a",
   "metadata": {},
   "source": [
    "## Plot characters personal names as a time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01079b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot characters' mentions as a time series relative to the position of the actor's occurrence in a book.\n",
    "\n",
    "def get_character_offsets(doc):\n",
    "    \"\"\"\n",
    "    For every character in a `doc` collect all the occurences offsets and store them into a list. \n",
    "    The function returns a dictionary that has actor lemma as a key and list of occurences as a value for every character.\n",
    "    \n",
    "    :param doc: Spacy NLP parsed document\n",
    "    :return: dict object in form\n",
    "        [('xxx', 622), ('yyy', 312), ('zzz', 286), ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    character_offsets = defaultdict(list)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            character_offsets[ent.lemma_].append(ent.start)\n",
    "            \n",
    "    return dict(character_offsets)\n",
    "\n",
    "character_occurences = get_character_offsets(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e79c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(character_occurences.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f94f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in list(character_occurences.keys()) if any([x.startswith(y) for y in ['Sun Yat', 'Chiang Kai']])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Keywords using noun chunks from the news article (file 'article.txt').\n",
    "# Spacy will pick some noun chunks that are not informative at all (e.g. we, what, who).\n",
    "# Try to find a way to remove non informative keywords.\n",
    "\n",
    "#article = read_file('data/article.txt')\n",
    "#doc = nlp(article)\n",
    "doc = processed_text\n",
    "\n",
    "keywords = Counter()\n",
    "for chunk in doc.noun_chunks:\n",
    "    if nlp.vocab[chunk.lemma_].prob < - 8: # probablity value -8 is arbitrarily selected threshold\n",
    "        keywords[chunk.lemma_] += 1\n",
    "\n",
    "keywords.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fffbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pyplot import hist\n",
    "from cycler import cycler\n",
    "\n",
    "NUM_BINS = 10\n",
    "\n",
    "def normalize(occurencies, normalization_constant):\n",
    "    return [o / float(len(processed_text)) for o in occurencies]\n",
    "\n",
    "def plot_character_timeseries(character_offsets, character_labels, normalization_constant=None):\n",
    "    \"\"\"\n",
    "    Plot characters' personal names specified in `character_labels` list as time series.\n",
    "    \n",
    "    :param character_offsets: dict object in form {'xxx': [123, 543, 4534], 'yyy': [205, 2111]}\n",
    "    :param character_labels: list of strings that should match some of the keys in `character_offsets`\n",
    "    :param normalization_constant: int\n",
    "    \"\"\" \n",
    "    x = [character_offsets[character_label] for character_label in character_labels] \n",
    "        \n",
    "    with plt.style.context('fivethirtyeight'):\n",
    "        plt.figure()\n",
    "        n, bins, patches = plt.hist(x, NUM_BINS, label=character_labels)\n",
    "        plt.clf()\n",
    "        \n",
    "        ax = plt.subplot(111)\n",
    "        for i, a in enumerate(n):\n",
    "            ax.plot([float(x) / (NUM_BINS - 1) for x in range(len(a))], a, label=character_labels[i])\n",
    "            \n",
    "        plt.rcParams['axes.prop_cycle'] = cycler(color=['r','k','c','b','y','m','g','#54a1FF'])\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "#plot_character_timeseries(character_occurences, ['darcy', 'bingley'], normalization_constant=len(processed_text))\n",
    "plot_character_timeseries(character_occurences, ['Mao Tse - tung', 'Chiang Kai - shek'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b51497",
   "metadata": {},
   "source": [
    "# 5. Important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a798959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_paragraphs(paragraphs, words):\n",
    "    \n",
    "    word_para = defaultdict(list)\n",
    "    for p in mao_paragraphs:\n",
    "        for w in words:\n",
    "            word_para[w].append(p.count(w))\n",
    "            \n",
    "    return dict(word_para)\n",
    "\n",
    "peasant_gentry_occurences = get_words_paragraphs(mao_paragraphs, ['peasant', 'gentry'])\n",
    "peasant_occurences = [v for k,v in peasant_gentry_occurences.items() if k == 'peasant'][0]\n",
    "gentry_occurences = [v for k,v in peasant_gentry_occurences.items() if k == 'gentry'][0]\n",
    "plt.plot(peasant_occurences)\n",
    "plt.plot(gentry_occurences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3bc359",
   "metadata": {},
   "source": [
    "[Moral](https://moralfoundations.org/other-materials/) words count in paragraphs that contain both `peasant` and `gentry`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca50f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "moral_words = []\n",
    "with open('D:/user/docs/NU/_Info6105/fp/fa.22/mfd.txt', \"r\") as input:\n",
    "    pair_lines = input.read().split(\"\\n\")\n",
    "for p in pair_lines:\n",
    "    moral_words.append(p.split('\\t')[0])\n",
    "print(moral_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edff84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_any_words_paragraphs(paragraphs, words):\n",
    "    \n",
    "    word_para = []\n",
    "    for p in tqdm(paragraphs):\n",
    "        total = 0\n",
    "        for w in words:\n",
    "            total += p.count(w)\n",
    "        word_para.append(total)\n",
    "            \n",
    "    return word_para\n",
    "\n",
    "moral_occurences = get_any_words_paragraphs(mao_paragraphs, moral_words)\n",
    "plt.plot(moral_occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(peasant_occurences)\n",
    "plt.plot(gentry_occurences)\n",
    "plt.plot([m / 4 for m in  moral_occurences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8dbfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_peasant_occurences = [bool(v) for v in peasant_occurences]\n",
    "bool_gentry_occurences = [bool(v) for v in gentry_occurences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db92786",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_moral = pd.DataFrame(list(zip(moral_occurences, gentry_occurences, peasant_occurences)), \n",
    "                        columns =['morals', 'gentry', 'peasants'])\n",
    "df_moral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d200c7d",
   "metadata": {},
   "source": [
    "# 6. Negative binomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fcfc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fml = 'morals ~ gentry + peasants + gentry:peasants'\n",
    "\n",
    "with pm.Model() as model:\n",
    "    pm.glm.GLM.from_formula(formula=fml, data=df_moral, family=pm.glm.families.NegativeBinomial())\n",
    "    trace = pm.sample(4000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03afb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs = [rv.name for rv in strip_derived_rvs(model.unobserved_RVs)]\n",
    "rvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traces(trace[1000:], varnames=rvs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3405b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform coefficients to recover parameter values\n",
    "np.exp(pm.summary(trace[1000:], varnames=rvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa4e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_moral2 = pd.DataFrame(list(zip(moral_occurences, bool_gentry_occurences, bool_peasant_occurences)), \n",
    "                        columns =['morals', 'gentry_p', 'peasants_p'])\n",
    "df_moral2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cd1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x='morals', row='gentry_p', col='peasants_p', data=df_moral2, kind='count', aspect=1.5)\n",
    "\n",
    "# Make x-axis ticklabels less crowded\n",
    "ax = g.axes[1, 0]\n",
    "labels = range(len(ax.get_xticklabels(which='both')))\n",
    "ax.set_xticks(labels[::5])\n",
    "ax.set_xticklabels(labels[::5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_moral2.groupby(['gentry_p', 'peasants_p'])['morals'].agg(['mean', 'var'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e66636",
   "metadata": {},
   "source": [
    "We see that mentioning `peasants` and/or `gentry` increases the presence of moral words to varying degrees. For each combination of `peasants` and `gentry`, the variance of moral words is higher than the mean. This suggests that a Poisson distrubution would be a poor fit for the data since the mean and variance of a Poisson distribution are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48602dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fml = 'morals ~ gentry_p + peasants_p + gentry_p:peasants_p'\n",
    "\n",
    "with pm.Model() as model:\n",
    "    pm.glm.GLM.from_formula(formula=fml, data=df_moral2, family=pm.glm.families.NegativeBinomial())\n",
    "    trace = pm.sample(2000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e4459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs = [rv.name for rv in strip_derived_rvs(model.unobserved_RVs)]\n",
    "print(rvs)\n",
    "plot_traces(trace[1000:], varnames=rvs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fadedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(pm.summary(trace[500:], varnames=rvs)[['mean','hdi_3%','hdi_97%']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec90134",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace[1000:]['Intercept'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9cdc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace[1000:]['peasants_p[T.True]'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7af987",
   "metadata": {},
   "source": [
    "Let's tally up more keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac328f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "enemy_words = ['tyrants', 'evil', 'landlords', 'landlord', 'gentry', 'enemy']\n",
    "friend_words = ['peasants', 'peasant', 'red', 'army', 'party', 'people', 'association', 'associations', 'masses', 'comrades']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56413f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "friend_occurences = get_any_words_paragraphs(mao_paragraphs, friend_words)\n",
    "enemy_occurences = get_any_words_paragraphs(mao_paragraphs, enemy_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58988fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(friend_occurences)\n",
    "plt.plot(enemy_occurences)\n",
    "plt.plot(moral_occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699a5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_friend_occurences = [bool(v) for v in friend_occurences]\n",
    "bool_enemy_occurences = [bool(v) for v in enemy_occurences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b325eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_moral3 = pd.DataFrame(list(zip(moral_occurences, bool_friend_occurences, bool_enemy_occurences)), \n",
    "                        columns =['morals', 'friend_p', 'enemy_p'])\n",
    "df_moral3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a708361",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x='morals', row='friend_p', col='enemy_p', data=df_moral3, kind='count', aspect=1.5)\n",
    "\n",
    "# Make x-axis ticklabels less crowded\n",
    "ax = g.axes[1, 0]\n",
    "labels = range(len(ax.get_xticklabels(which='both')))\n",
    "ax.set_xticks(labels[::5])\n",
    "ax.set_xticklabels(labels[::5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d4e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_moral3.groupby(['friend_p', 'enemy_p'])['morals'].agg(['mean', 'var'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10797aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fml = 'morals ~ friend_p + enemy_p + enemy_p:friend_p'\n",
    "\n",
    "with pm.Model() as model:\n",
    "    pm.glm.GLM.from_formula(formula=fml, data=df_moral3, family=pm.glm.families.NegativeBinomial())\n",
    "    trace = pm.sample(10000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs = [rv.name for rv in strip_derived_rvs(model.unobserved_RVs)]\n",
    "print(rvs)\n",
    "plot_traces(trace[1000:], varnames=rvs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614c30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(pm.summary(trace[1000:], varnames=rvs)[['mean','hdi_3%','hdi_97%']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e48c991",
   "metadata": {},
   "source": [
    "Does not make sense! The baseline is 3, that agrees with the statistics of oour dataset, but friend_p[T.True] should be times 6, enemy_p[T.True] should be times 8, and enemy_p[T.True]:friend_p[T.True] should be times 24!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a58284",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_interaction:\n",
    "    az.plot_forest(\n",
    "        trace,\n",
    "        model_names=[\"Mao\"],\n",
    "        var_names=['Intercept', 'friend_p[T.True]', 'enemy_p[T.True]', 'enemy_p[T.True]:friend_p[T.True]'],\n",
    "        combined=True,\n",
    "        figsize=(8, 4)\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26da613",
   "metadata": {},
   "source": [
    "Try with sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e497492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "moral_occurences = get_any_words_paragraphs(mao_sentences, moral_words)\n",
    "friend_occurences = get_any_words_paragraphs(mao_sentences, friend_words)\n",
    "enemy_occurences = get_any_words_paragraphs(mao_sentences, enemy_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca7263",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_friend_occurences = [bool(v) for v in friend_occurences]\n",
    "bool_enemy_occurences = [bool(v) for v in enemy_occurences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faed870",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_moral4 = pd.DataFrame(list(zip(moral_occurences, bool_friend_occurences, bool_enemy_occurences)), \n",
    "                        columns =['morals', 'friend_p', 'enemy_p'])\n",
    "df_moral4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c3a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x='morals', row='friend_p', col='enemy_p', data=df_moral4, kind='count', aspect=1.5)\n",
    "\n",
    "# Make x-axis ticklabels less crowded\n",
    "ax = g.axes[1, 0]\n",
    "labels = range(len(ax.get_xticklabels(which='both')))\n",
    "ax.set_xticks(labels[::5])\n",
    "ax.set_xticklabels(labels[::5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_moral4.groupby(['friend_p', 'enemy_p'])['morals'].agg(['mean', 'var'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fml = 'morals ~ friend_p + enemy_p + enemy_p:friend_p'\n",
    "\n",
    "with pm.Model() as model4:\n",
    "    pm.glm.GLM.from_formula(formula=fml, data=df_moral4, family=pm.glm.families.NegativeBinomial())\n",
    "    trace = pm.sample(5000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52969d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs = [rv.name for rv in strip_derived_rvs(model4.unobserved_RVs)]\n",
    "print(rvs)\n",
    "with model4:\n",
    "    plot_traces(trace[1000:], varnames=rvs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae86432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(pm.summary(trace[1000:], varnames=rvs)[['mean','hdi_3%','hdi_97%']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dabae0f",
   "metadata": {},
   "source": [
    "# 7. TextRank\n",
    "```\n",
    "pip install sentence-transformers\n",
    "pip install -U typing-extensions\n",
    "pip install cython\n",
    "pip install tokenizers-0.10.3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec820da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = SentenceTransformer('stsb-roberta-large') #1,024 dims\n",
    "#model2 = SentenceTransformer('bert-base-nli-mean-tokens') #768 dims\n",
    "bmodel = SentenceTransformer('all-MiniLM-L6-v2') #384 dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede23ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings = bmodel.encode(mao_sentences)\n",
    "bert_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955acc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the embeddings\n",
    "for i, (sentence, embedding) in enumerate(zip(mao_sentences, bert_embeddings)):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")\n",
    "    if i == 3: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46065d0c",
   "metadata": {},
   "source": [
    "We compute a [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between all vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0921cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = bert_embeddings[0]\n",
    "b = bert_embeddings[1]\n",
    "cos_sim = (a @ b.T) / (np.linalg.norm(a)*np.linalg.norm(b))\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd457432",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e731549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "#M = np.zeros((1933, 1933))\n",
    "M = np.eye(1933)\n",
    "\n",
    "for i,a in tqdm(enumerate(bert_embeddings)):\n",
    "    for j,b in enumerate(bert_embeddings):\n",
    "        if i != j:\n",
    "            M[i,j] = (a @ b.T) / (np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32937b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "\n",
    "similarity_graph = networkx.from_numpy_array(M)\n",
    "similarity_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03834d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "networkx.draw_networkx(similarity_graph, node_color='lime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce470d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = networkx.pagerank(similarity_graph)\n",
    "nx_ranked_sentences = sorted(((score, index) for index, score \n",
    "                                            in scores.items()), \n",
    "                          reverse=True)\n",
    "nx_ranked_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1dbc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mao_sentences[1609]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d17e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mao_sentences[642]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ebeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(mao_sentences[nx_ranked_sentences[i][1]])\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a6d95",
   "metadata": {},
   "source": [
    "But what I would really like to do is to submit a search query sentence, and *then* get the most relvant sentences!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee28f4",
   "metadata": {},
   "source": [
    "# 8. kMedoids\n",
    "\n",
    "K-Medoids is a clustering algorithm resembling the K-Means clustering technique. It falls under the category of unsupervised machine learning. It majorly differs from the K-Means algorithm in terms of the way it selects the clusters’ centres. The former selects the average of a cluster’s points as its centre (which may or may not be one of the data points) while the latter always picks the actual data points from the clusters as their centres (also known as ‘exemplars’ or ‘medoids’). K-Medoids also differs in this respect from the K-Medians algorithm whic,h is the same as K-means, except that it chooses the medians (instead of means) of the clusters as centres.\n",
    "\n",
    "Mean of the data points is a measure that gets highly affected by the extreme points. So in K-Means algorithm, the centroid may get shifted to a wrong position and hence result in incorrect clustering if the data has outliers because then other points will move away from  . On the contrary, a medoid in the K-Medoids algorithm is the most central element of the cluster, such that its distance from other points is minimum. Since medoids do not get influenced by extremities, the K-Medoids algorithm is more robust to outliers and noise than K-Means algorithm.\n",
    "\n",
    "Randomly pick `k` points from the input data to create `k` clusters. The correctness of the choice of k’s value can be assessed using methods such as the Silhouette method. Each data point gets assigned recusively to the cluster to which its nearest medoid belongs. The algorithm is as follows:\n",
    "\n",
    "For each data point of cluster i, its distance from all other data points is computed and added. The point of ith cluster for which the computed sum of distances from other points is minimal is assigned as the medoid for that cluster. Repeat until convergence is reached i.e. the medoids stop moving.\n",
    "\n",
    "The complexity of the K-Medoids algorithm comes to $O(N^2CT)$ where N, C and T denote the number of data points, number of clusters and number of iterations respectively. With similar notations, the complexity K-Means algorithm can be given as $O(NCT)$.\n",
    "\n",
    "Two known implementations:\n",
    "- [pip install scikit-learn-extra](https://scikit-learn-extra.readthedocs.io/en/latest/generated/sklearn_extra.cluster.KMedoids.html#sklearn_extra.cluster.KMedoids)\n",
    "- [pip install kmedoids](https://pypi.org/project/kmedoids/)\n",
    "\n",
    "Let's use scikit-learn-extra.\n",
    "\n",
    "Also, instead of using all 3844 attributes of the dataset, we will use Principal Component Analysis (PCA)  to reduce the dimensions of features set such that most of the useful information is covered.\n",
    "\n",
    "We also import a module for standardizing the dataset i.e. rescaling the data such that its has mean of 0 and standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac2e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64002539",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1f11e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_digits = len(np.unique(digits.target))\n",
    "num_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa653496",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(bert_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd888f",
   "metadata": {},
   "source": [
    "Standardize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf18e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings_std = scale(bert_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0189577",
   "metadata": {},
   "source": [
    "[Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) constructs new components by linear combinations of original features. ‘n_components’ parameter denotes the number of newly formed components to be considered. fit_transform() method fits the PCA models and performs dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c4def6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings_red = PCA(n_components=2).fit_transform(bert_embeddings_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc52f4d8",
   "metadata": {},
   "source": [
    "Plot the decision boundaries for each cluster. Assign a different color to each for differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c856ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.02 #step size of the mesh \n",
    "#Minimum and maximum x-coordinates\n",
    "xmin, xmax = bert_embeddings_red[:, 0].min() - 1, bert_embeddings_red[:, 0].max() + 1\n",
    "#Minimum and maximum y-coordinates\n",
    "ymin, ymax = bert_embeddings_red[:, 1].min() - 1, bert_embeddings_red[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(xmin, xmax, h), np.arange(ymin, ymax, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28479b82",
   "metadata": {},
   "source": [
    "Define an array of K-Medoids variants to be used. We have used  three different distance metrics (Manhattan distance, Euclidean distance and Cosine dissimilarity/distance) for computing the distance of each data point from every other data point while selecting the medoid. \n",
    "\n",
    "Manhattan distance is usually preferred over the more common Euclidean distance when there is high dimensionality in the data. Hamming distance is used to measure the distance between categorical variables, and the Cosine distance metric is mainly used to find the amount of similarity between two data points.\n",
    "\n",
    "The parameters specified in the KMedoids() method have the following significance:\n",
    "\n",
    "- metric – distance metric to be used (default: ‘euclidean’)\n",
    "- n_clusters – number of clusters to be formed and hence the number of medoids (one per cluster) (default value: 8)\n",
    "- init – ‘heuristic’ method used for medoid initialization. For each data point, itd distance from all other points is computed and the distances are summed up. N_clusters number of points for which such a sum of distances are minimum, are chosen as medoids.\n",
    "- max_iter – maximum number of the algorithm’s iterations to be performed when fitting the data\n",
    "\n",
    "The KMedoids() method of `scikit-learn-extra` by default uses the PAM (Partition Around Medoids) algorithm for finding the medoids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa0c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "     (\n",
    "         KMedoids(metric=\"manhattan\", n_clusters=num_digits, \n",
    "         init=\"heuristic\", max_iter=2),\"Manhattan metric\",\n",
    "     ),\n",
    "     (\n",
    "         KMedoids(metric=\"euclidean\", n_clusters=num_digits,  \n",
    "         init=\"heuristic\", max_iter=2),\"Euclidean metric\",\n",
    "     ),\n",
    "     (KMedoids(metric=\"cosine\", n_clusters=num_digits, init=\"heuristic\", \n",
    "      max_iter=2), \"Cosine metric\", ),\n",
    " ]\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92d6b9",
   "metadata": {},
   "source": [
    "Initialize the number of rows and columns of the plot for plotting subplots of each of the three metrics’ results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of rows = integer(ceiling(number of model variants/2))\n",
    "num_rows = int(np.ceil(len(models) / 2.0))\n",
    "#number of columns\n",
    "num_cols = 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f4b88",
   "metadata": {},
   "source": [
    "Fit each of the model variants to the data and plot the resultant clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca3b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear the current figure first (if any)\n",
    "plt.clf()\n",
    "\n",
    "#Initialize dimensions of the plot\n",
    "plt.figure(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dc65b6",
   "metadata": {},
   "source": [
    "The `models` array contains three tuples, each having a model variant’s parameters and its descriptive text. We iterate through each of the tuples, fit the data to the model and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (model, description) in enumerate(models):\n",
    "    # Fit each point in the mesh to the model\n",
    "    model.fit(bert_embeddings_red)\n",
    "    \n",
    "    #Predict the labels for points in the mesh\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Put the result  into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "   \n",
    "    #Subplot for the ith model variant\n",
    "    plt.subplot(num_cols, num_rows, i + 1)\n",
    "   \n",
    "    #Display the subplot\n",
    "    plt.imshow(\n",
    "         Z,    #data to be plotted\n",
    "         interpolation=\"nearest\",\n",
    "        #bounding box coordinates (left,right,bottom,top)\n",
    "         extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "         cmap=plt.cm.Paired,  #colormap\n",
    "         aspect=\"auto\", #aspect ratio of the axes\n",
    "         origin=\"lower\",  #set origin as lower left corner of the axes\n",
    "    )\n",
    "    plt.plot(\n",
    "         bert_embeddings_red[:, 0], bert_embeddings_red[:, 1], \"k.\", markersize=2, alpha=0.3\n",
    "    )\n",
    "        \n",
    "    # Plot the centroids as white cross marks\n",
    "    centroids = model.cluster_centers_\n",
    "    plt.scatter(\n",
    "         centroids[:, 0],\n",
    "         centroids[:, 1],\n",
    "         marker=\"x\",\n",
    "         s=169,  #marker’s size (points^2)\n",
    "         linewidths=3, #width of boundary lines\n",
    "         color=\"w\",  #white color for centroids markings\n",
    "         zorder=10,  #drawing order of axes\n",
    "    )\n",
    "    \n",
    "    #describing text of the tuple will be title of the subplot\n",
    "    plt.title(description)  \n",
    "    plt.xlim(xmin, xmax)  #limits of x-coordinates\n",
    "    plt.ylim(ymin, ymax)  #limits of y-coordinates\n",
    "    plt.xticks(())   \n",
    "    plt.yticks(())\n",
    "    \n",
    "#Upper title of the whole plot\n",
    "plt.suptitle(\n",
    "    #Text to be displayed\n",
    "    \"K-Medoids algorithm implemented with different metrics\\n\\n\",\n",
    "    fontsize=20,  #size of the fonts\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327bef0",
   "metadata": {},
   "source": [
    "Without dimensionality reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d9573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = []\n",
    "for i, (model, description) in enumerate(models):\n",
    "    model.fit(bert_embeddings_std)\n",
    "    centroids.append(model.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0766624",
   "metadata": {},
   "source": [
    "Identify the centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_indeces = []\n",
    "\n",
    "for i in range(10):\n",
    "    c = centroids[2][i]\n",
    "    for j in range(bert_embeddings_std.shape[0]):\n",
    "        if np.allclose(c, bert_embeddings_std[j]):\n",
    "            centroid_indeces.append(j)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84781b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8186131",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in centroid_indeces:\n",
    "    print(i, ' -------')\n",
    "    print(mao_sentences[i])\n",
    "    print('---------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38857d5",
   "metadata": {},
   "source": [
    "# 9. Latent Dirichlet Allocation\n",
    "In natural language processing, [Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (LDA) is a generative statistical model that explains a set of observations through unobserved groups, and each group explains why some parts of the data are similar. The LDA is an example of a [topic model](https://en.wikipedia.org/wiki/Topic_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "     \n",
    "# spacy for lemmatization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e198e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8367e4b",
   "metadata": {},
   "source": [
    "Tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a45e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "mao_words = list(sent_to_words(mao_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c2d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(mao_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[mao_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[mao_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed480de",
   "metadata": {},
   "source": [
    "Remove Stopwords, Make Bigrams and Lemmatize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d77ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db10e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "mao_words_nostops = remove_stopwords(mao_words)\n",
    "\n",
    "# Form Bigrams\n",
    "mao_words_bigrams = make_bigrams(mao_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "mao_lemmatized = lemmatization(mao_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(mao_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c0238",
   "metadata": {},
   "source": [
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(mao_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = mao_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3bbefe",
   "metadata": {},
   "source": [
    "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
    "\n",
    "For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7587ec7",
   "metadata": {},
   "source": [
    "If you want to see what word a given id corresponds to, pass the id as a key to the dictionary.\n",
    "```\n",
    "id2word[0]\n",
    "```\n",
    "\n",
    "Or, you can see a human-readable form of the corpus itself.\n",
    "```\n",
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6470c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e077e8af",
   "metadata": {},
   "source": [
    "### Building the Topic Model\n",
    "We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
    "\n",
    "Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.\n",
    "\n",
    "chunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daed9085",
   "metadata": {},
   "source": [
    "### View the topics in LDA model\n",
    "The above LDA model is built with 20 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "\n",
    "You can see the keywords for each topic and the weightage(importance) of each keyword using lda_model.print_topics() as shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e85a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f319c79",
   "metadata": {},
   "source": [
    "It means the top 10 keywords that contribute to the furst topic are: `army` `give`, `divide`, `difficulty`, `emerge`, `approach`, `main`, `reason`, `directive`, `tremendous` and the weight of `army` on topic 0 is 0.0119.\n",
    "\n",
    "The weights reflect how important a keyword is to that topic.\n",
    "\n",
    "Looking at these keywords, can you guess what this topic could be? You may summarise it as...\n",
    "\n",
    "Likewise, can you go through the remaining topic keywords and judge what the topic may be.\n",
    "\n",
    "Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is. In my experience, topic coherence score, in particular, has been more helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d970499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=mao_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef00f8f",
   "metadata": {},
   "source": [
    "### Visualize the topics-keywords\n",
    "Now that the LDA model is built, the next step is to examine the produced topics and the associated keywords. There is no better tool than pyLDAvis package’s interactive chart and is designed to work well with jupyter notebooks.\n",
    "```\n",
    "pip install future\n",
    "pip install pyLDAvis\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64f31cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd9e517",
   "metadata": {},
   "source": [
    "Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n",
    "\n",
    "A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n",
    "\n",
    "A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n",
    "\n",
    "If you move the cursor over one of the bubbles, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f6aa2",
   "metadata": {},
   "source": [
    "### Optimizing coherence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e849f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82680a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=mao_lemmatized, \n",
    "                                                        start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c7f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae1c3e1",
   "metadata": {},
   "source": [
    "If the coherence score seems to keep increasing, it may make better sense to pick the model that gave the highest CV before flattening out. This is exactly the case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e57ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e4b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = model_list[3]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f6ced9",
   "metadata": {},
   "source": [
    "### Finding the dominant topic in each sentence\n",
    "One of the practical application of topic modeling is to determine what topic a given document is about.\n",
    "\n",
    "To find that, we find the topic number that has the highest percentage contribution in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=mao_sentences):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=mao_sentences)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98fe535",
   "metadata": {},
   "source": [
    "### Find the most representative document for each topic\n",
    "Sometimes just the topic keywords may not be enough to make sense of what a topic is about. So, to help with understanding the topic, you can find the documents a given topic has contributed to the most and infer the topic by reading that document. Whew!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf = pd.concat([sent_topics_sorteddf, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aa62b0",
   "metadata": {},
   "source": [
    "### Topic distribution across documents\n",
    "Finally, we want to understand the volume and distribution of topics in order to judge how widely it was discussed. The below table exposes that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82109c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f07091",
   "metadata": {},
   "source": [
    "# 10. Spacy parse tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6867d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find words (adjectives) that describe Mr. Darcy.\n",
    "\n",
    "def get_character_adjectives(doc, character_lemma):\n",
    "    \"\"\"\n",
    "    Find all the adjectives related to `character_lemma` in `doc`\n",
    "    \n",
    "    :param doc: Spacy NLP parsed document\n",
    "    :param character_lemma: string object\n",
    "    :return: list of adjectives related to `character_lemma`\n",
    "    \"\"\"\n",
    "    \n",
    "    adjectives = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.lemma_ == character_lemma:\n",
    "            for token in ent.subtree:\n",
    "                if token.pos_ == 'ADJ': # Replace with if token.dep_ == 'amod':\n",
    "                    adjectives.append(token.lemma_)\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.lemma_ == character_lemma:\n",
    "            if ent.root.dep_ == 'nsubj':\n",
    "                for child in ent.root.head.children:\n",
    "                    if child.dep_ == 'acomp':\n",
    "                        adjectives.append(child.lemma_)\n",
    "    \n",
    "    return adjectives\n",
    "\n",
    "print(get_character_adjectives(processed_text, 'Chiang Kai - shek'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c7f53",
   "metadata": {},
   "source": [
    "Chiang Kai-shek, also known as Chiang Chung-cheng and Jiang Jieshi, was a Chinese Nationalist politician, revolutionary, and military leader who served as the leader of the Republic of China (ROC) from 1928 to his death in 1975 – until 1949 in mainland China and from then on in Taiwan. After his rule was confined to Taiwan following his defeat by Mao Zedong in the Chinese Civil War, he continued claiming to head the legitimate Chinese government in exile.\n",
    "\n",
    "No wonder Mao thinks he is a `western` agent ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf71489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find characters that are 'talking', 'saying', 'doing' the most. Find the relationship between \n",
    "# entities and corresponding root verbs.\n",
    "\n",
    "character_verb_counter = Counter()\n",
    "VERB_LEMMA = 'say'\n",
    "\n",
    "for ent in processed_text.ents:\n",
    "    if ent.label_ == 'PERSON' and ent.root.head.lemma_ == VERB_LEMMA:\n",
    "        character_verb_counter[ent.text] += 1\n",
    "\n",
    "print(character_verb_counter.most_common(10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b094cd",
   "metadata": {},
   "source": [
    "## Entities extraction\n",
    "\n",
    "The extraction of a single word entity from a sentence is not a tough task. We can easily do this with the help of parts of speech (POS) tags. The nouns and the proper nouns would be our entities.\n",
    "\n",
    "However, when an entity spans across multiple words, then POS tags alone are not sufficient. We need to parse the dependency tree of the sentence.\n",
    "\n",
    "To build a knowledge graph, the most important things are the nodes and the edges between them.\n",
    "\n",
    "These nodes are going to be the entities that are present in the Wikipedia sentences. Edges are the relationships connecting these entities to one another. We will extract these elements in an unsupervised manner, i.e., we will use the grammar of the sentences.\n",
    "\n",
    "The main idea is to go through a sentence and extract the subject and the object as and when they are encountered. However, there are a few challenges ⁠— an entity can span across multiple words, eg., “red wine”, and the dependency parsers tag only the individual words as subjects or objects.\n",
    "\n",
    "The function belowt extracts the subject and the object (entities) from a sentence while also overcoming the challenges mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16864304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "    ## chunk 1\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "    prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "  #############################################################\n",
    "  \n",
    "    for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "            prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85606fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entities(\"Dino wants to marry Dua Lipa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "entity_pairs = []\n",
    "\n",
    "for i in tqdm(mao_sentences):\n",
    "    entity_pairs.append(get_entities(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2090f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pairs[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33dc56",
   "metadata": {},
   "source": [
    "## Relations extraction\n",
    "\n",
    "Entity extraction is half the job. To build a knowledge graph, we need edges to connect the nodes (entities) to one another. These edges are the relations between a pair of nodes.\n",
    "\n",
    "Our hypothesis is that the predicate is actually the main verb in a sentence.\n",
    "\n",
    "For example, in the sentence – We are the Chinese revolution, the verb is `are` and this is what we are going to use as the predicate for the triple generated from this sentence.\n",
    "\n",
    "The function below is capable of capturing such predicates from the sentences. Here, I have used spaCy’s rule-based matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904dcb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "def get_relation(sent):\n",
    "\n",
    "  doc = nlp(sent)\n",
    "\n",
    "  # Matcher class object \n",
    "  matcher = Matcher(nlp.vocab)\n",
    "\n",
    "  #define the pattern \n",
    "  pattern = [{'DEP':'ROOT'}, \n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},  \n",
    "            {'POS':'ADJ','OP':\"?\"}] \n",
    "\n",
    "  matcher.add(\"matching_1\", None, pattern) \n",
    "\n",
    "  matches = matcher(doc)\n",
    "  k = len(matches) - 1\n",
    "\n",
    "  span = doc[matches[k][1]:matches[k][2]] \n",
    "\n",
    "  return(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4131d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_relation(\"Dino married Dua Lipa and they lived happily ever after\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = [get_relation(i) for i in tqdm(mao_sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8c1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(relations).value_counts()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c96066",
   "metadata": {},
   "source": [
    "Resource Description Framework (RDF) is a well know data model for information extraction and was adopted as a World Wide Web Consortium recommendation in 1999 as a general method for conceptual description or modeling of information that is implemented in web resources. \n",
    "\n",
    "The RDF relates entities by the subject-predicate-object format where the subject and object are related to one another by the predicate. Later it was also used in knowledge management applications involving structured text contents. \n",
    "\n",
    "A [semantic triple](https://en.wikipedia.org/wiki/Semantic_triple) in a text sentence is defined as a relation between subject and object, the relation being the predicate.The aim here is to extract sets of the form {subject, predicate, object} out of syntactically parsed sentences.The triple is a minimal representation for information without losing the context.\n",
    "\n",
    "Can you extract semantic triples from the code above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165625f6",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb347a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = '. '.join(mao_sentences)\n",
    "mao_text = nltk.Text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f5d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.tokenize.wordpunct_tokenize(text)  # produces a list, like split()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d35780",
   "metadata": {},
   "outputs": [],
   "source": [
    "mao_text.dispersion_plot([\"peasant\", \"gentry\", \"Red Army\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213385e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Creat the variable sa to hold the VADER lexicon object \n",
    "sa = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efca3a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the lexicon contents\n",
    "# There are over 7500 tokens in the lexicon\n",
    "sa.lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d5e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.polarity_scores('I love Dua Lipa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02872e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_sentiment_scores = []\n",
    "for s in mao_sentences:\n",
    "    scores = sa.polarity_scores(s)\n",
    "    sentences_sentiment_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f72769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sentiment = [s['pos'] for s in sentences_sentiment_scores]\n",
    "plt.plot(positive_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1dc8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([s['neg'] for s in sentences_sentiment_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77278175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.fft import rfft, irfft, rfftfreq\n",
    "from scipy import pi, signal, fftpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_pass(s, threshold=2e4):\n",
    "    fourier = rfft(s)\n",
    "    #frequencies = rfftfreq(s.size, d=2e-3 / s.size)\n",
    "    frequencies = rfftfreq(len(s), d=2e-3 / len(s))\n",
    "    display(1/frequencies)\n",
    "    fourier[frequencies > threshold] = 0\n",
    "    return irfft(fourier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fba387",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sentiment_low_pass = low_pass(positive_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a6e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(positive_sentiment_low_pass)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
